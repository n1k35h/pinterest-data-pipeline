{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "30b2f2f8-afd2-469c-81ef-f2bb239b5fb0",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.functions import *\n",
    "import urllib\n",
    "\n",
    "# Define the path to the Delta table\n",
    "delta_table_path = \"dbfs:/user/hive/warehouse/authentication_credentials\"\n",
    "\n",
    "# Read the Delta table to a Spark DataFrame\n",
    "aws_keys_df = spark.read.format(\"delta\").load(delta_table_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bde6998a-69bb-4828-bd2f-a26af085c830",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Get the AWS access key and secret key from the spark dataframe\n",
    "ACCESS_KEY = aws_keys_df.select('Access key ID').collect()[0]['Access key ID']\n",
    "SECRET_KEY = aws_keys_df.select('Secret access key').collect()[0]['Secret access key']\n",
    "# Encode the secrete key\n",
    "ENCODED_SECRET_KEY = urllib.parse.quote(string=SECRET_KEY, safe=\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3beeeb19-ce49-4917-865e-3e7b26246660",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "-- Disable format checks during the reading of Delta tables\n",
    "SET spark.databricks.delta.formatCheck.enabled=false\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "412c1f5b-940e-4778-91cd-5b33dcacae27",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "dbutils.fs.rm(\"/tmp/kinesis/_checkpoints/\", True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "dc422921-8b08-4687-bdb6-17c06b777ae8",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def read_stream_data(data_name):\n",
    "    df = spark \\\n",
    "        .readStream \\\n",
    "        .format('kinesis') \\\n",
    "        .option('streamName',f'streaming-0e0816526d11-{data_name}') \\\n",
    "        .option('initialPosition','earliest') \\\n",
    "        .option('region','us-east-1') \\\n",
    "        .option('awsAccessKey', ACCESS_KEY) \\\n",
    "        .option('awsSecretKey', SECRET_KEY) \\\n",
    "        .load()\n",
    "    df = df.selectExpr(\"CAST(data as STRING)\") # extracting the data in the stream\n",
    "    return df\n",
    "\n",
    "def write_stream_data(df, data_name):\n",
    "    df.writeStream \\\n",
    "        .format(\"delta\") \\\n",
    "        .outputMode(\"append\") \\\n",
    "        .option(\"checkpointLocation\", \"/tmp/kinesis/_checkpoints/\") \\\n",
    "        .table(f\"0e0816526d11_{data_name}_table\")\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "87a88e3e-b391-4893-ab63-0d08cfd4d64e",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "pin_streaming_schema = StructType([\n",
    "    StructField(\"index\", StringType(), True),\n",
    "    StructField(\"unique_id\", StringType(), True),\n",
    "    StructField(\"title\", StringType(), True),\n",
    "    StructField(\"description\", StringType(), True),\n",
    "    StructField(\"poster_name\", StringType(), True),\n",
    "    StructField(\"follower_count\", StringType(), True),\n",
    "    StructField(\"tag_list\", StringType(), True),\n",
    "    StructField(\"is_image_or_video\", StringType(), True),\n",
    "    StructField(\"image_src\", StringType(), True),\n",
    "    StructField(\"downloaded\", IntegerType(), True),\n",
    "    StructField(\"save_location\", StringType(), True),\n",
    "    StructField(\"category\", StringType(), True)\n",
    "])\n",
    "\n",
    "df_pin = read_stream_data(\"pin\")\n",
    "\n",
    "# extracting the fields from a JSON string and converting them into columns of a DataFrame\n",
    "df_pin = df_pin.withColumn(\"jsonData\",from_json(col(\"data\"),pin_streaming_schema)) \\\n",
    "                   .select(\"jsonData.*\")\n",
    "\n",
    "# cleaning Pinterest dataframe\n",
    "df_pin = df_pin.replace(\"No description available Story format\", None)\n",
    "df_pin = df_pin.replace(\"null\", None)\n",
    "df_pin = df_pin.replace(\"User Info Error\", None)\n",
    "df_pin = df_pin.replace(\"Image src error\", None)\n",
    "df_pin = df_pin.replace(\"N,o, ,T,a,g,s, ,A,v,a,i,l,a,b,l,e\", None)\n",
    "df_pin = df_pin.replace(\"No Title Data Available\", None)\n",
    "\n",
    "# transforming the 'follower_count' col from string to integer and checking if the value matches a pattern that contains either k or M \n",
    "df_pin = df_pin.withColumn(\"follower_count\", when(\n",
    "    col(\"follower_count\").rlike(\"\\d+k\"),(regexp_extract(col(\"follower_count\"),\"(\\d+)\",1).cast(\"integer\") * 1000)).when(col(\"follower_count\").rlike(\"\\d+M\"),(regexp_extract(col(\"follower_count\"), \"(\\d+)\", 1).cast(\"integer\") * 1000000))\n",
    "# otherwise, if it doesn't match it leaves the full integer value\n",
    ".otherwise(col(\"follower_count\").cast(\"integer\")))\n",
    "\n",
    "# cleaning the 'save_location' column by removing 'Local save in ' text and just leaving the path for the 'save_location' column\n",
    "df_pin = df_pin.withColumn(\"save_location\", regexp_replace(\"save_location\", \"Local save in \", \"\"))\n",
    "\n",
    "# Renaming the column\n",
    "df_pin = df_pin.withColumnRenamed(\"index\", \"ind\")\n",
    "\n",
    "# removes the row that contains Null value\n",
    "df_pin = df_pin.na.drop()\n",
    "\n",
    "# rearranging the Pinterest columns\n",
    "reorder_col = [\"ind\", \"unique_id\", \"title\", \"description\", \"follower_count\", \"poster_name\", \"tag_list\", \"is_image_or_video\", \"image_src\", \"save_location\", \"category\"]\n",
    "df_pin = df_pin.select(reorder_col)\n",
    "\n",
    "display(df_pin)\n",
    "write_stream_data(df_pin, \"pin\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b32425d6-cedd-4f81-88d4-e52b0fd2b692",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "geo_streaming_schema = StructType([\n",
    "    StructField(\"ind\", StringType(), True),\n",
    "    StructField(\"timestamp\", StringType(), True),\n",
    "    StructField(\"latitude\", StringType(), True),\n",
    "    StructField(\"longitude\", StringType(), True),\n",
    "    StructField(\"country\", StringType(), True)\n",
    "])\n",
    "\n",
    "df_geo = read_stream_data(\"geo\")\n",
    "\n",
    "# extracting the fields from a JSON string and converting them into columns of a DataFrame\n",
    "df_geo = df_geo.withColumn(\"jsonData\",from_json(col(\"data\"),geo_streaming_schema)) \\\n",
    "                   .select(\"jsonData.*\")\n",
    "\n",
    "# created a new column containing latitude and longitude\n",
    "df_geo = df_geo.withColumn(\"coordinates\", array(col(\"latitude\"), col(\"longitude\")))\n",
    "\n",
    "# dropping columns\n",
    "df_geo = df_geo.drop(\"latitude\", \"longitude\")\n",
    "df_geo = df_geo.withColumn(\"timestamp\", col(\"timestamp\").cast(\"timestamp\"))\n",
    "\n",
    "geo_reorder_col = [\"ind\", \"country\", \"coordinates\", \"timestamp\"]\n",
    "df_geo = df_geo.select(geo_reorder_col)\n",
    "\n",
    "display(df_geo)\n",
    "write_stream_data(df_geo, \"geo\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "faece7f5-2d48-4c08-88b2-545a9974ad47",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "user_streaming_schema = StructType([\n",
    "    StructField(\"ind\", StringType(), True),\n",
    "    StructField(\"first_name\", StringType(), True),\n",
    "    StructField(\"last_name\", StringType(), True),\n",
    "    StructField(\"age\", StringType(), True),\n",
    "    StructField(\"date_joined\", StringType(), True)\n",
    "])\n",
    "\n",
    "df_user = read_stream_data(\"user\")\n",
    "\n",
    "# extracting the fields from a JSON string and converting them into columns of a DataFrame\n",
    "df_user = df_user.withColumn(\"jsonData\",from_json(col(\"data\"),user_streaming_schema)) \\\n",
    "                   .select(\"jsonData.*\")\n",
    "\n",
    "df_user = df_user.withColumn(\"user_name\", concat(col(\"first_name\"),lit(\" \"),col(\"last_name\")))\n",
    "df_user = df_user.drop(\"first_name\",\"last_name\",\"index\")\n",
    "df_user = df_user.withColumn(\"date_joined\", col(\"date_joined\").cast(\"timestamp\"))\n",
    "df_user = df_user.dropDuplicates([\"user_name\", \"age\", \"date_joined\"])\n",
    "\n",
    "user_reorder_col = [\"ind\",\"user_name\",\"age\",\"date_joined\"]\n",
    "df_user = df_user.select(user_reorder_col)\n",
    "\n",
    "display(df_user)\n",
    "write_stream_data(df_user, \"user\")"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 2715316850887608,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "streaming_kinesis_databricks",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
